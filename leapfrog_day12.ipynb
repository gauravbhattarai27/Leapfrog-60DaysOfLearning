{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "**Authors**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova  \n",
        "**Published**: NAACL 2019  \n",
        "**By**: Google AI Language\n",
        "\n",
        "---\n",
        "\n",
        "## Overview of BERT\n",
        "\n",
        "**BERT** stands for **Bidirectional Encoder Representations from Transformers**.\n",
        "\n",
        "It introduced a new method for **pre-training language models** to deeply understand the **context of language from both directions (left and right)**.\n",
        "\n",
        "### Key Innovations:\n",
        "- Deep **bidirectional** context understanding.\n",
        "- **Pre-training + Fine-tuning** approach.\n",
        "- State-of-the-art results on **11 NLP tasks** with the **same architecture**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Was BERT Revolutionary?\n",
        "\n",
        "**Before BERT**:\n",
        "- Language models were **unidirectional**.\n",
        "- Limited context understanding.\n",
        "- Required **task-specific architectures** for downstream tasks.\n",
        "\n",
        "**With BERT**:\n",
        "- **Bidirectional Transformers** to capture context from both sides.\n",
        "- Unified architecture for **multiple tasks**.\n",
        "- Fine-tuning with minimal changes.\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Details\n",
        "\n",
        "BERT is based on the **Transformer encoder** from \"Attention is All You Need\" (2017).\n",
        "\n",
        "| Component | Details |\n",
        "|----------|---------|\n",
        "| Model | Transformer Encoder only |\n",
        "| Layers | 12 (Base), 24 (Large) |\n",
        "| Hidden Size | 768 (Base), 1024 (Large) |\n",
        "| Self-Attention Heads | 12 (Base), 16 (Large) |\n",
        "| Parameters | 110M (Base), 340M (Large) |\n",
        "\n",
        "---\n",
        "\n",
        "## Pre-training Tasks\n",
        "\n",
        "### 1. Masked Language Modeling (MLM)\n",
        "- Randomly masks 15% of input tokens.\n",
        "- Model predicts the original value of masked tokens using surrounding context.\n",
        "\n",
        "**Example**:  \n",
        "Input: \"I like to [MASK] ice cream.\"  \n",
        "Target: Predict \"[MASK]\" as \"eat\"\n",
        "\n",
        "**Goal**: Learn deep **bidirectional context**.\n",
        "\n",
        "### 2. Next Sentence Prediction (NSP)\n",
        "- Predicts whether sentence B is the **actual next sentence** of sentence A.\n",
        "\n",
        "**Example**:  \n",
        "Sentence A: \"He went to the store.\"  \n",
        "Sentence B: \"He bought a gallon of milk.\" → IsNext  \n",
        "Random sentence B: \"Penguins are flightless birds.\" → NotNext\n",
        "\n",
        "**Goal**: Understand **sentence relationships**.\n",
        "\n",
        "---\n",
        "\n",
        "## Input Representation\n",
        "\n",
        "To support both single and sentence-pair tasks, BERT uses a combined input format:\n",
        "\n",
        "| Component | Explanation |\n",
        "|----------|-------------|\n",
        "| [CLS] Token | First token of every input sequence. Its final hidden state is used for classification. |\n",
        "| [SEP] Token | Separates sentence A and sentence B. |\n",
        "| Token Embeddings | WordPiece tokenized input |\n",
        "| Segment Embeddings | Identify sentence A or B |\n",
        "| Position Embeddings | Represent word positions in sequence |\n",
        "\n",
        "---\n",
        "\n",
        "## Fine-tuning on Downstream Tasks\n",
        "\n",
        "After pre-training, BERT is fine-tuned by adding a task-specific output layer.\n",
        "\n",
        "### Example Tasks:\n",
        "\n",
        "| Task | Input Format | Output |\n",
        "|------|--------------|--------|\n",
        "| Sentiment Classification | [CLS] + sentence | Classification label from [CLS] token |\n",
        "| Question Answering | [CLS] + question + [SEP] + context | Start and end token positions |\n",
        "| Named Entity Recognition | Tokens | Label per token |\n",
        "\n",
        "Fine-tuning is performed with just a few epochs and minimal architectural changes.\n",
        "\n",
        "---\n",
        "\n",
        "## Results\n",
        "\n",
        "BERT achieved **state-of-the-art** results on multiple NLP benchmarks:\n",
        "\n",
        "- GLUE (General Language Understanding Evaluation)\n",
        "- SQuAD v1.1 and v2.0 (Question Answering)\n",
        "- SWAG (Common Sense Inference)\n",
        "\n",
        "---\n",
        "\n",
        "## Key Innovations Summary\n",
        "\n",
        "| Innovation | Benefit |\n",
        "|-----------|---------|\n",
        "| Masked Language Model | Allows deep bidirectional understanding |\n",
        "| Next Sentence Prediction | Captures inter-sentence relationships |\n",
        "| Single architecture | Used across tasks without major changes |\n",
        "| Transfer Learning for NLP | Pre-train once, fine-tune for many tasks |\n",
        "\n",
        "---\n",
        "\n",
        "## BERT Summary Table\n",
        "\n",
        "| Aspect | BERT |\n",
        "|--------|------|\n",
        "| Full Name | Bidirectional Encoder Representations from Transformers |\n",
        "| Architecture | Transformer Encoder |\n",
        "| Directionality | Deeply Bidirectional |\n",
        "| Pre-training Tasks | MLM and NSP |\n",
        "| Fine-tuning | Task-specific head on top |\n",
        "| Model Sizes | BERT-Base (110M), BERT-Large (340M) |\n",
        "| Training Data | BooksCorpus + English Wikipedia |\n",
        "| Tokenizer | WordPiece with 30k vocabulary |\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations of BERT\n",
        "\n",
        "- **Expensive to pre-train** (compute-heavy).\n",
        "- NSP was later questioned (RoBERTa removed it).\n",
        "- Not ideal for real-time or low-latency applications due to size.\n",
        "\n",
        "---\n",
        "\n",
        "## Follow-up Models\n",
        "\n",
        "| Model | Improvement |\n",
        "|-------|-------------|\n",
        "| RoBERTa | Removed NSP, trained longer and on more data |\n",
        "| DistilBERT | Smaller and faster BERT |\n",
        "| ALBERT | Parameter-sharing to reduce memory |\n",
        "| BioBERT, ClinicalBERT | Domain-specific variants for biomedical/clinical texts |\n",
        "\n",
        "---\n",
        "\n",
        "## Final Thoughts\n",
        "\n",
        "BERT changed the landscape of NLP by:\n",
        "- Introducing a unified **pre-training + fine-tuning** framework.\n",
        "- Achieving excellent results on a **variety of NLP tasks**.\n",
        "- Inspiring many **improvements and new models** in the field.\n",
        "\n",
        "It is a **foundation model** that paved the way for future large-scale pre-trained models in NLP.\n"
      ],
      "metadata": {
        "id": "EEdkluU3wijG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNZa4pXYwns-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}