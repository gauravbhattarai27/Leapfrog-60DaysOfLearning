{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# XGBoost: Extreme Gradient Boosting\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "**XGBoost (Extreme Gradient Boosting)** is an efficient and scalable implementation of gradient boosting framework by Tianqi Chen and Carlos Guestrin. It has gained popularity due to its performance in machine learning competitions and real-world applications.\n",
        "\n",
        "It is based on **Gradient Boosted Decision Trees (GBDT)**, and incorporates several optimizations to improve speed and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Key Concepts\n",
        "\n",
        "### 2.1 Gradient Boosting\n",
        "\n",
        "Gradient Boosting is an **ensemble method** that builds models sequentially. Each new model corrects the errors made by the previous ones. It uses the gradient of the loss function to minimize prediction error.\n",
        "\n",
        "Basic steps:\n",
        "\n",
        "1. Initialize the model with a constant value.\n",
        "2. Compute pseudo-residuals (negative gradients of the loss).\n",
        "3. Fit a weak learner (typically a decision tree) on the residuals.\n",
        "4. Update the model by adding the new learner.\n",
        "5. Repeat for a fixed number of iterations or until convergence.\n",
        "\n",
        "### 2.2 Decision Trees as Base Learners\n",
        "\n",
        "XGBoost uses **CART (Classification and Regression Trees)** as base learners. Each tree is built to minimize a differentiable loss function (e.g., Mean Squared Error for regression, Log Loss for classification).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Objective Function in XGBoost\n",
        "\n",
        "The overall objective function is:\n",
        "\n",
        "```\n",
        "Obj = L(θ) + Ω(θ)\n",
        "```\n",
        "\n",
        "* **L(θ)**: Training loss function (e.g., squared error, logistic loss)\n",
        "* **Ω(θ)**: Regularization term to prevent overfitting\n",
        "\n",
        "Where:\n",
        "\n",
        "```\n",
        "Ω(f) = γT + 0.5λ∑w²\n",
        "```\n",
        "\n",
        "* T = number of leaves\n",
        "* w = score on each leaf\n",
        "* γ = complexity cost per leaf\n",
        "* λ = L2 regularization term\n",
        "\n",
        "### 3.1 Taylor Expansion\n",
        "\n",
        "XGBoost uses a **second-order Taylor approximation** of the loss function for optimization:\n",
        "\n",
        "```\n",
        "Obj ≈ ∑[gᵢf(xᵢ) + 0.5hᵢf(xᵢ)²] + Ω(f)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* gᵢ = first-order gradient (∂L/∂f(xᵢ))\n",
        "* hᵢ = second-order gradient (∂²L/∂f(xᵢ)²)\n",
        "\n",
        "This allows more accurate and stable updates.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Split Finding\n",
        "\n",
        "### 4.1 Greedy Algorithm\n",
        "\n",
        "To build the tree, XGBoost uses a greedy algorithm to search for the best split by maximizing the **Gain** (reduction in loss):\n",
        "\n",
        "```\n",
        "Gain = 0.5 * [ (G_L² / (H_L + λ)) + (G_R² / (H_R + λ)) - (G² / (H + λ)) ] - γ\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* G\\_L, H\\_L = gradient and hessian of left child\n",
        "* G\\_R, H\\_R = gradient and hessian of right child\n",
        "* G, H = total gradient and hessian\n",
        "* γ = regularization term\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Regularization\n",
        "\n",
        "XGBoost incorporates regularization into the tree-building process:\n",
        "\n",
        "* **L1 and L2 regularization** on leaf weights\n",
        "* **Tree complexity penalty** using the number of leaves\n",
        "* Helps reduce overfitting and improve generalization\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Handling Missing Values\n",
        "\n",
        "XGBoost automatically learns how to handle missing values by finding the best direction (left/right) for missing data during training.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Shrinkage (Learning Rate)\n",
        "\n",
        "After each boosting step, predictions are updated as:\n",
        "\n",
        "```\n",
        "fᵗ(x) = fᵗ⁻¹(x) + η * f_new(x)\n",
        "```\n",
        "\n",
        "Where **η (eta)** is the **learning rate** (typically between 0.01 to 0.3). It controls how much each new tree contributes.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Column Subsampling\n",
        "\n",
        "Similar to Random Forests, XGBoost supports:\n",
        "\n",
        "* **Feature subsampling per tree** or **per level**\n",
        "* Helps reduce overfitting and improves performance\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Parallel and Distributed Computing\n",
        "\n",
        "XGBoost supports:\n",
        "\n",
        "* **Parallelization** of split finding during tree construction\n",
        "* **Distributed training** over multiple machines\n",
        "* **Out-of-core computation** for handling large datasets\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Advantages of XGBoost\n",
        "\n",
        "* High prediction accuracy\n",
        "* Regularization to prevent overfitting\n",
        "* Handles missing values internally\n",
        "* Efficient memory usage and training speed\n",
        "* Scalable for large datasets\n",
        "* Works well with both structured and tabular data\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Applications\n",
        "\n",
        "* Classification (binary, multi-class)\n",
        "* Regression\n",
        "* Ranking problems\n",
        "* Time series prediction\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Summary\n",
        "\n",
        "XGBoost is a powerful and flexible machine learning algorithm that builds upon the principles of gradient boosting with system-level optimizations. Its success lies in combining a robust theoretical foundation with practical enhancements like regularization, second-order gradients, and efficient computation.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ErJUppBK5mWn"
      }
    }
  ]
}